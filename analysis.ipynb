{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import supervision as sv\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from inference2 import loadModel, inference4, inference2, inference3\n",
    "from img2pdf import readPDF, savePDF\n",
    "from createGroundTruth import create_ground_truth_dict\n",
    "import time\n",
    "import torchvision\n",
    "from torchvision.ops import box_iou\n",
    "import torch\n",
    "import pytorch_lightning\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-101\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training examples: 130\n",
      "Number of validation examples: 32\n",
      "Number of test examples: 19\n"
     ]
    }
   ],
   "source": [
    "## CocoDetection Class \n",
    "\n",
    "# settings\n",
    "ANNOTATION_FILE_NAME = r\"result.json\"\n",
    "TRAIN_DIRECTORY = os.path.join(r\"dataset2\", r\"train\")\n",
    "VAL_DIRECTORY = os.path.join(r\"dataset2\", r\"val\")\n",
    "TEST_DIRECTORY = os.path.join(r\"dataset2\", r\"test\")\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_directory_path: str,\n",
    "        image_processor,\n",
    "        train: bool = True\n",
    "    ):\n",
    "        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
    "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "\n",
    "        return pixel_values, target\n",
    "    \n",
    "TRAIN_DATASET = CocoDetection(\n",
    "    image_directory_path=TRAIN_DIRECTORY,\n",
    "    image_processor=image_processor,\n",
    "    train=True)\n",
    "VAL_DATASET = CocoDetection(\n",
    "    image_directory_path=VAL_DIRECTORY,\n",
    "    image_processor=image_processor,\n",
    "    train=False)\n",
    "TEST_DATASET = CocoDetection(\n",
    "    image_directory_path=TEST_DIRECTORY,\n",
    "    image_processor=image_processor,\n",
    "    train=False)\n",
    "\n",
    "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
    "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
    "print(\"Number of test examples:\", len(TEST_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training examples: 145\n",
      "Number of validation examples: 18\n",
      "Number of test examples: 18\n"
     ]
    }
   ],
   "source": [
    "# ## CocoDetection Class \n",
    "\n",
    "# # settings\n",
    "# ANNOTATION_FILE_NAME = r\"result.json\"\n",
    "# TRAIN_DIRECTORY = os.path.join(r\"dataset\", r\"train\")\n",
    "# VAL_DIRECTORY = os.path.join(r\"dataset\", r\"val\")\n",
    "# TEST_DIRECTORY = os.path.join(r\"dataset\", r\"test\")\n",
    "\n",
    "# class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         image_directory_path: str,\n",
    "#         image_processor,\n",
    "#         train: bool = True\n",
    "#     ):\n",
    "#         annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
    "#         super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
    "#         self.image_processor = image_processor\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
    "#         image_id = self.ids[idx]\n",
    "#         annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "#         encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "#         pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "#         target = encoding[\"labels\"][0]\n",
    "\n",
    "#         return pixel_values, target\n",
    "    \n",
    "# TRAIN_DATASET = CocoDetection(\n",
    "#     image_directory_path=TRAIN_DIRECTORY+'/images',\n",
    "#     image_processor=image_processor,\n",
    "#     train=True)\n",
    "# VAL_DATASET = CocoDetection(\n",
    "#     image_directory_path=VAL_DIRECTORY+'/images',\n",
    "#     image_processor=image_processor,\n",
    "#     train=False)\n",
    "# TEST_DATASET = CocoDetection(\n",
    "#     image_directory_path=TEST_DIRECTORY+'/images',\n",
    "#     image_processor=image_processor,\n",
    "#     train=False)\n",
    "\n",
    "# print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
    "# print(\"Number of validation examples:\", len(VAL_DATASET))\n",
    "# print(\"Number of test examples:\", len(TEST_DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"facebook/detr-resnet-50\"\n",
    "\n",
    "# Best Performing Model\n",
    "MODEL_PATH = \"models/DETR-run7\"\n",
    "\n",
    "# Doesnt Work\n",
    "# MODEL_101 = 'facebook/detr-resnet-101'\n",
    "# CHECKPOINT_101 = 'facebook/detr-resnet-101'\n",
    "\n",
    "# Older Model\n",
    "# MODEL_PATH = \"models/DETR-run4\"\n",
    "\n",
    "\n",
    "## Load Model\n",
    "def loadModel(MODEL_PATH, CHECKPOINT):\n",
    "    model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n",
    "    image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)\n",
    "    return model, image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrForObjectDetection\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Initialize the model architecture\n",
    "model, image_processor = loadModel(MODEL_PATH=MODEL_PATH, CHECKPOINT=CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model, image_processor = loadModel(MODEL_PATH=MODEL_101, CHECKPOINT=CHECKPOINT_101)\n",
    "\n",
    "# Load your checkpoint\n",
    "checkpoint = torch.load(\"models/DETR-run7/ModelCheckpoints2/detr-epoch=47-val_loss=0.53.ckpt\", map_location='cpu')\n",
    "\n",
    "# # Get the state dict\n",
    "state_dict = checkpoint['state_dict']\n",
    "\n",
    "# # # Remove the 'model.model.' prefix from the state dict keys\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state_dict.items():\n",
    "    name = k.replace(\"model.model.\", \"\")\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "# # # Load the modified state dict\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Ground Truth Dictionary\n",
    "ground_truth = create_ground_truth_dict('dataset2/test/result.json')\n",
    "\n",
    "# For Older models\n",
    "# ground_truth = create_ground_truth_dict('dataset/test/images/result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images/4fdca17b-drawing_7.png': {'boxes': [[109.90697674418603,\n",
       "    136.0,\n",
       "    151.02325581395348,\n",
       "    176.32558139534885],\n",
       "   [542.4186046511628,\n",
       "    364.51162790697674,\n",
       "    634.9302325581396,\n",
       "    407.2093023255814],\n",
       "   [426.1860465116279,\n",
       "    460.9767441860465,\n",
       "    535.3023255813954,\n",
       "    487.86046511627916],\n",
       "   [214.27906976744188,\n",
       "    517.9069767441861,\n",
       "    290.9767441860465,\n",
       "    547.1627906976744]],\n",
       "  'labels': [3, 1, 0, 2]},\n",
       " 'images/8318502d-drawing_15.png': {'boxes': [[411.16279069767444,\n",
       "    68.0,\n",
       "    448.3255813953489,\n",
       "    105.95348837209303],\n",
       "   [608.8372093023256,\n",
       "    370.83720930232556,\n",
       "    699.7674418604652,\n",
       "    406.4186046511627],\n",
       "   [43.48837209302325,\n",
       "    529.7674418604652,\n",
       "    118.6046511627907,\n",
       "    555.0697674418604],\n",
       "   [389.8139534883721,\n",
       "    476.79069767441865,\n",
       "    439.62790697674416,\n",
       "    498.9302325581396],\n",
       "   [219.81395348837205,\n",
       "    143.90697674418604,\n",
       "    290.18604651162786,\n",
       "    162.09302325581393]],\n",
       "  'labels': [3, 1, 0, 2, 2]},\n",
       " 'images/eef413ab-drawing_22.png': {'boxes': [[672.8837209302327,\n",
       "    77.48837209302324,\n",
       "    706.8837209302327,\n",
       "    116.23255813953489],\n",
       "   [552.6976744186046,\n",
       "    376.3720930232558,\n",
       "    633.3488372093022,\n",
       "    413.5348837209302],\n",
       "   [588.2790697674419,\n",
       "    455.44186046511635,\n",
       "    678.4186046511627,\n",
       "    477.5813953488372],\n",
       "   [229.30232558139534,\n",
       "    510.79069767441854,\n",
       "    313.90697674418607,\n",
       "    535.3023255813953],\n",
       "   [126.51162790697674,\n",
       "    330.51162790697674,\n",
       "    195.30232558139534,\n",
       "    355.81395348837214],\n",
       "   [193.72093023255812,\n",
       "    122.55813953488372,\n",
       "    259.3488372093023,\n",
       "    148.65116279069767]],\n",
       "  'labels': [3, 1, 0, 2, 2, 2]},\n",
       " 'images/2067578c-drawing_24.png': {'boxes': [[50.60465116279069,\n",
       "    93.30232558139537,\n",
       "    91.72093023255813,\n",
       "    131.2558139534884],\n",
       "   [47.441860465116285,\n",
       "    522.6511627906976,\n",
       "    132.04651162790697,\n",
       "    549.5348837209302],\n",
       "   [423.0232558139535,\n",
       "    459.3953488372093,\n",
       "    497.34883720930236,\n",
       "    486.27906976744185],\n",
       "   [105.16279069767441,\n",
       "    417.4883720930232,\n",
       "    176.32558139534882,\n",
       "    441.20930232558135],\n",
       "   [374.0, 427.7674418604651, 445.95348837209303, 455.44186046511635]],\n",
       "  'labels': [3, 0, 0, 2, 2]},\n",
       " 'images/449d473c-drawing_25.png': {'boxes': [[41.906976744186046,\n",
       "    60.883720930232556,\n",
       "    77.48837209302326,\n",
       "    101.2093023255814],\n",
       "   [449.9069767441861, 527.3953488372093, 530.5581395348838, 562.186046511628],\n",
       "   [43.48837209302325,\n",
       "    527.3953488372093,\n",
       "    149.4418604651163,\n",
       "    548.7441860465116],\n",
       "   [62.46511627906977,\n",
       "    384.27906976744185,\n",
       "    113.86046511627907,\n",
       "    398.51162790697674],\n",
       "   [231.67441860465118,\n",
       "    422.2325581395349,\n",
       "    265.6744186046512,\n",
       "    438.04651162790697],\n",
       "   [282.27906976744185,\n",
       "    280.69767441860466,\n",
       "    320.2325581395349,\n",
       "    289.39534883720927],\n",
       "   [472.83720930232556,\n",
       "    188.97674418604652,\n",
       "    507.62790697674416,\n",
       "    205.5813953488372],\n",
       "   [673.6744186046512,\n",
       "    240.37209302325584,\n",
       "    716.3720930232557,\n",
       "    256.1860465116279]],\n",
       "  'labels': [3, 1, 0, 2, 2, 2, 2, 2]},\n",
       " 'images/f601361a-drawing_42.png': {'boxes': [[46.65116279069767,\n",
       "    86.18604651162792,\n",
       "    85.3953488372093,\n",
       "    125.72093023255815],\n",
       "   [443.5813953488372,\n",
       "    522.6511627906976,\n",
       "    526.6046511627907,\n",
       "    559.0232558139535],\n",
       "   [652.3255813953489,\n",
       "    456.2325581395349,\n",
       "    738.5116279069769,\n",
       "    490.2325581395349],\n",
       "   [57.72093023255814,\n",
       "    523.4418604651163,\n",
       "    162.88372093023256,\n",
       "    547.1627906976744],\n",
       "   [232.46511627906978,\n",
       "    407.99999999999994,\n",
       "    268.04651162790697,\n",
       "    423.8139534883721],\n",
       "   [242.74418604651163,\n",
       "    242.7441860465117,\n",
       "    294.13953488372096,\n",
       "    258.55813953488376],\n",
       "   [223.7674418604651,\n",
       "    170.7906976744186,\n",
       "    275.95348837209303,\n",
       "    185.8139534883721],\n",
       "   [596.9767441860465,\n",
       "    180.27906976744188,\n",
       "    642.0465116279071,\n",
       "    195.30232558139534]],\n",
       "  'labels': [3, 1, 1, 0, 2, 2, 2, 2]},\n",
       " 'images/3108be30-drawing_43.png': {'boxes': [[115.31266149870802,\n",
       "    466.68992248062017,\n",
       "    152.29974160206723,\n",
       "    502.5891472868217],\n",
       "   [501.5012919896642,\n",
       "    765.8501291989663,\n",
       "    582.0025839793283,\n",
       "    802.8372093023256],\n",
       "   [77.23772609819122,\n",
       "    775.6408268733851,\n",
       "    165.3540051679587,\n",
       "    799.5736434108528],\n",
       "   [574.3875968992248,\n",
       "    88.11627906976744,\n",
       "    723.4237726098191,\n",
       "    125.10335917312662],\n",
       "   [932.2919896640828,\n",
       "    257.8217054263566,\n",
       "    1059.5710594315246,\n",
       "    298.0723514211886]],\n",
       "  'labels': [3, 1, 0, 2, 2]},\n",
       " 'images/a34b8e8d-drawing_44.png': {'boxes': [[123.81395348837209,\n",
       "    75.72093023255813,\n",
       "    162.69767441860466,\n",
       "    115.62790697674419],\n",
       "   [448.18604651162786,\n",
       "    524.9302325581394,\n",
       "    538.232558139535,\n",
       "    568.9302325581394],\n",
       "   [93.11627906976744,\n",
       "    609.8604651162791,\n",
       "    184.18604651162792,\n",
       "    635.4418604651163],\n",
       "   [81.86046511627906,\n",
       "    639.5348837209302,\n",
       "    173.95348837209303,\n",
       "    660.0000000000001],\n",
       "   [105.3953488372093,\n",
       "    462.5116279069768,\n",
       "    206.69767441860466,\n",
       "    492.186046511628]],\n",
       "  'labels': [3, 1, 0, 0, 2]},\n",
       " 'images/92e4c80c-drawing_45.png': {'boxes': [[80.65116279069767,\n",
       "    77.48837209302324,\n",
       "    117.8139534883721,\n",
       "    114.65116279069767],\n",
       "   [642.046511627907, 459.3953488372093, 725.860465116279, 496.5581395348837],\n",
       "   [60.093023255813954,\n",
       "    521.860465116279,\n",
       "    151.8139534883721,\n",
       "    543.9999999999999],\n",
       "   [75.90697674418605,\n",
       "    428.55813953488365,\n",
       "    134.4186046511628,\n",
       "    446.7441860465116],\n",
       "   [266.46511627906983,\n",
       "    400.8837209302325,\n",
       "    301.2558139534884,\n",
       "    414.3255813953489],\n",
       "   [240.37209302325581,\n",
       "    168.41860465116278,\n",
       "    294.13953488372096,\n",
       "    184.2325581395349],\n",
       "   [617.5348837209303,\n",
       "    290.9767441860465,\n",
       "    663.3953488372093,\n",
       "    309.95348837209303]],\n",
       "  'labels': [3, 1, 0, 2, 2, 2, 2]},\n",
       " 'images/a55f419c-drawing_48.png': {'boxes': [[63.25581395348837,\n",
       "    68.0,\n",
       "    106.74418604651163,\n",
       "    106.74418604651163],\n",
       "   [238.7906976744186,\n",
       "    520.2790697674418,\n",
       "    328.93023255813955,\n",
       "    551.906976744186],\n",
       "   [57.72093023255814,\n",
       "    508.41860465116275,\n",
       "    165.25581395348837,\n",
       "    536.0930232558139]],\n",
       "  'labels': [3, 1, 0]},\n",
       " 'images/b6c7e8be-drawing_56.png': {'boxes': [[51.395348837209305,\n",
       "    132.83720930232556,\n",
       "    90.13953488372094,\n",
       "    170.0],\n",
       "   [602.5116279069767,\n",
       "    460.18604651162786,\n",
       "    690.2790697674419,\n",
       "    494.97674418604646],\n",
       "   [30.046511627906977,\n",
       "    521.860465116279,\n",
       "    120.97674418604655,\n",
       "    541.6279069767442],\n",
       "   [128.09302325581396,\n",
       "    519.4883720930233,\n",
       "    216.6511627906977,\n",
       "    540.8372093023256],\n",
       "   [470.4651162790698,\n",
       "    189.7674418604651,\n",
       "    507.6279069767441,\n",
       "    204.79069767441862],\n",
       "   [672.8837209302327,\n",
       "    241.16279069767444,\n",
       "    715.5813953488372,\n",
       "    256.9767441860465]],\n",
       "  'labels': [3, 1, 0, 0, 2, 2]},\n",
       " 'images/5acab2e6-drawing_64.png': {'boxes': [[57.72093023255814,\n",
       "    175.53488372093022,\n",
       "    98.04651162790697,\n",
       "    212.69767441860463],\n",
       "   [41.906976744186046,\n",
       "    529.7674418604652,\n",
       "    131.25581395348837,\n",
       "    550.3255813953489],\n",
       "   [226.93023255813958,\n",
       "    311.5348837209303,\n",
       "    294.9302325581396,\n",
       "    340.79069767441854],\n",
       "   [224.55813953488374,\n",
       "    170.7906976744186,\n",
       "    291.7674418604651,\n",
       "    198.46511627906978],\n",
       "   [625.4418604651163,\n",
       "    247.48837209302326,\n",
       "    710.046511627907,\n",
       "    274.3720930232558]],\n",
       "  'labels': [3, 0, 2, 2, 2]},\n",
       " 'images/2466f98b-drawing_67.png': {'boxes': [[109.11627906976744,\n",
       "    392.9767441860465,\n",
       "    147.86046511627907,\n",
       "    429.3488372093024],\n",
       "   [586.6976744186047, 433.30232558139534, 669.7209302325582, 476.0],\n",
       "   [28.465116279069765,\n",
       "    496.55813953488376,\n",
       "    120.97674418604653,\n",
       "    517.9069767441861],\n",
       "   [137.58139534883722,\n",
       "    495.7674418604651,\n",
       "    230.88372093023253,\n",
       "    518.6976744186046],\n",
       "   [37.16279069767442,\n",
       "    526.6046511627907,\n",
       "    125.72093023255815,\n",
       "    549.5348837209302],\n",
       "   [143.11627906976744,\n",
       "    528.9767441860465,\n",
       "    231.67441860465118,\n",
       "    548.7441860465116],\n",
       "   [212.69767441860466,\n",
       "    283.86046511627904,\n",
       "    264.8837209302326,\n",
       "    299.6744186046511],\n",
       "   [410.3720930232558,\n",
       "    73.53488372093022,\n",
       "    462.5581395348837,\n",
       "    90.93023255813952],\n",
       "   [660.232558139535,\n",
       "    257.7674418604651,\n",
       "    703.7209302325582,\n",
       "    272.7906976744186]],\n",
       "  'labels': [3, 1, 0, 0, 0, 0, 2, 2, 2]},\n",
       " 'images/af0c0b68-drawing_76.png': {'boxes': [[47.441860465116285,\n",
       "    371.6279069767442,\n",
       "    81.44186046511628,\n",
       "    411.16279069767444],\n",
       "   [383.48837209302326,\n",
       "    68.79069767441861,\n",
       "    417.4883720930233,\n",
       "    110.69767441860466],\n",
       "   [631.7674418604652, 317.8604651162791, 717.953488372093, 352.6511627906977],\n",
       "   [45.860465116279066,\n",
       "    520.2790697674418,\n",
       "    150.23255813953486,\n",
       "    543.9999999999999],\n",
       "   [229.30232558139534,\n",
       "    510.79069767441854,\n",
       "    313.90697674418607,\n",
       "    536.0930232558139],\n",
       "   [128.09302325581396,\n",
       "    329.7209302325581,\n",
       "    196.88372093023258,\n",
       "    358.1860465116279],\n",
       "   [609.6279069767442,\n",
       "    234.83720930232556,\n",
       "    679.9999999999999,\n",
       "    258.5581395348837],\n",
       "   [384.2790697674419,\n",
       "    228.51162790697674,\n",
       "    481.5348837209303,\n",
       "    255.39534883720927],\n",
       "   [609.6279069767442,\n",
       "    128.88372093023256,\n",
       "    679.9999999999999,\n",
       "    153.39534883720933],\n",
       "   [191.34883720930233,\n",
       "    123.34883720930233,\n",
       "    260.13953488372096,\n",
       "    148.6511627906977],\n",
       "   [478.3720930232558,\n",
       "    373.20930232558146,\n",
       "    561.3953488372093,\n",
       "    400.8837209302326]],\n",
       "  'labels': [3, 3, 1, 0, 2, 2, 2, 2, 2, 2, 2]},\n",
       " 'images/11913d58-drawing_88.png': {'boxes': [[39.53488372093023,\n",
       "    351.06976744186045,\n",
       "    75.11627906976744,\n",
       "    389.813953488372],\n",
       "   [46.65116279069767,\n",
       "    66.41860465116278,\n",
       "    83.81395348837209,\n",
       "    105.16279069767441],\n",
       "   [279.1162790697674,\n",
       "    529.7674418604652,\n",
       "    363.72093023255815,\n",
       "    559.0232558139536],\n",
       "   [626.2325581395348,\n",
       "    409.58139534883725,\n",
       "    732.1860465116279,\n",
       "    429.3488372093024],\n",
       "   [217.4418604651163,\n",
       "    518.6976744186046,\n",
       "    278.3255813953489,\n",
       "    542.4186046511627],\n",
       "   [626.2325581395348,\n",
       "    266.4651162790698,\n",
       "    713.2093023255813,\n",
       "    290.1860465116279]],\n",
       "  'labels': [3, 3, 1, 0, 2, 2]},\n",
       " 'images/17bd5f92-drawing_106.png': {'boxes': [[595.3953488372093,\n",
       "    89.34883720930233,\n",
       "    630.9767441860465,\n",
       "    127.30232558139534],\n",
       "   [617.5348837209303,\n",
       "    386.6511627906977,\n",
       "    695.8139534883721,\n",
       "    419.86046511627904],\n",
       "   [46.65116279069767,\n",
       "    515.5348837209302,\n",
       "    136.7906976744186,\n",
       "    535.3023255813953],\n",
       "   [76.69767441860465,\n",
       "    260.9302325581395,\n",
       "    173.953488372093,\n",
       "    285.44186046511624],\n",
       "   [430.1395348837209, 279.906976744186, 529.767441860465, 305.2093023255814],\n",
       "   [426.9767441860466,\n",
       "    108.32558139534883,\n",
       "    498.139534883721,\n",
       "    136.7906976744186]],\n",
       "  'labels': [3, 1, 0, 2, 2, 2]},\n",
       " 'images/cc5b592c-drawing_113.png': {'boxes': [[136.7906976744186,\n",
       "    263.30232558139534,\n",
       "    172.37209302325581,\n",
       "    302.04651162790697],\n",
       "   [64.04651162790698,\n",
       "    515.5348837209302,\n",
       "    169.2093023255814,\n",
       "    539.2558139534883],\n",
       "   [237.20930232558138,\n",
       "    506.83720930232556,\n",
       "    303.6279069767442,\n",
       "    534.5116279069767],\n",
       "   [627.8139534883721, 311.5348837209303, 714.0, 336.046511627907],\n",
       "   [350.27906976744185,\n",
       "    166.046511627907,\n",
       "    449.1162790697675,\n",
       "    192.93023255813955]],\n",
       "  'labels': [3, 0, 2, 2, 2]},\n",
       " 'images/e616d66b-drawing_123.png': {'boxes': [[48.23255813953489,\n",
       "    140.74418604651163,\n",
       "    90.93023255813955,\n",
       "    178.69767441860466],\n",
       "   [258.5581395348837,\n",
       "    529.7674418604652,\n",
       "    344.74418604651163,\n",
       "    559.8139534883721],\n",
       "   [481.5348837209302,\n",
       "    407.99999999999994,\n",
       "    585.1162790697674,\n",
       "    430.1395348837209],\n",
       "   [605.6744186046512,\n",
       "    407.99999999999994,\n",
       "    709.2558139534884,\n",
       "    424.6046511627907],\n",
       "   [215.86046511627907,\n",
       "    519.4883720930233,\n",
       "    278.3255813953488,\n",
       "    544.7906976744185],\n",
       "   [627.0232558139535,\n",
       "    265.6744186046511,\n",
       "    711.6279069767443,\n",
       "    289.39534883720927]],\n",
       "  'labels': [3, 1, 0, 0, 2, 2]},\n",
       " 'images/a01b4a37-drawing_168.png': {'boxes': [[121.76744186046511,\n",
       "    64.46511627906978,\n",
       "    159.62790697674419,\n",
       "    107.44186046511629],\n",
       "   [513.6744186046512,\n",
       "    425.6744186046511,\n",
       "    551.5348837209302,\n",
       "    467.6279069767441],\n",
       "   [450.2325581395349,\n",
       "    512.6511627906978,\n",
       "    534.1395348837209,\n",
       "    547.4418604651163],\n",
       "   [108.46511627906975,\n",
       "    526.9767441860465,\n",
       "    221.02325581395348,\n",
       "    552.5581395348836],\n",
       "   [104.3720930232558,\n",
       "    563.813953488372,\n",
       "    218.97674418604652,\n",
       "    584.2790697674418],\n",
       "   [386.79069767441865,\n",
       "    431.81395348837214,\n",
       "    461.48837209302326,\n",
       "    456.37209302325584]],\n",
       "  'labels': [3, 3, 1, 0, 0, 2]}}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'bar-scale', 1: 'color-stamp', 2: 'detail-labels', 3: 'north-sign'}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = TEST_DATASET.coco.cats\n",
    "id2label = {k: v['name'] for k,v in categories.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference Function\n",
    "IMAGE_FOLDER = 'Temp/images'\n",
    "CONFIDENCE_THRESHOLD = 0.6\n",
    "IOU_THRESHOLD = 0.7\n",
    "\n",
    "def inference(image_folder, CONFIDENCE_THRESHOLD, IOU_THRESHOLD):\n",
    "    results_dict = {}\n",
    "    \n",
    "    for img in os.listdir(image_folder):\n",
    "        IMAGE_PATH = os.path.join(image_folder, img)\n",
    "        print(f\"Processing {IMAGE_PATH}\")\n",
    "\n",
    "        image = cv2.imread(IMAGE_PATH)\n",
    "        inputs = image_processor(images=image, return_tensors='pt')\n",
    "\n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Get ground truth for this image\n",
    "        target = ground_truth.get(img, {'boxes': torch.empty((0, 4)), 'labels': torch.empty((0,), dtype=torch.long)})\n",
    "        # target = {k: torch.tensor(v).to(model.device) for k, v in target.items()}\n",
    "        # target = {k: v.clone().detach().to(model.device) for k, v in target.items()}\n",
    "        target = {k: v for k, v in target.items()}\n",
    "        # print(target)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Post-process\n",
    "            target_sizes = torch.tensor([image.shape[:2]]).to(model.device)\n",
    "            results = image_processor.post_process_object_detection(\n",
    "                outputs=outputs,\n",
    "                threshold=CONFIDENCE_THRESHOLD,\n",
    "                target_sizes=target_sizes\n",
    "            )[0]\n",
    "        \n",
    "        # detections = sv.Detections.from_transformers(transformers_results=results).with_nms(threshold=IOU_THRESHOLD)\n",
    "        detections = sv.Detections.from_transformers(transformers_results=results)\n",
    "        labels = [f\"{id2label[class_id]} {confidence:.2f}\" for _, confidence, class_id, _ in detections]\n",
    "        print(labels)\n",
    "        box_annotator = sv.BoxAnnotator()\n",
    "        frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
    "        Image.fromarray(frame).save(f\"Temp/results/annotated_{img}\", format='PNG')\n",
    "        # results.append([IMAGE_PATH.replace('Temp/', ''), result])\n",
    "        results_dict[IMAGE_PATH.replace('Temp/', '')] = results\n",
    "        # all_labels = {1, 2, 3, 4}\n",
    "        \n",
    "        # results_dict[IMAGE_PATH.replace('Temp/', '')], results_dict['missing_labels'] = results, (all_labels - set(results['labels'].detach))\n",
    "        # print(results['labels'])}\n",
    "        # labels_df['missing_labels'] = labels_df['labels'].apply(lambda x: all_labels - x)\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Temp/images\\11913d58-drawing_88.png\n",
      "['LABEL_1 0.53', 'LABEL_3 0.91', 'LABEL_2 0.70', 'LABEL_2 0.64', 'LABEL_3 0.78', 'LABEL_2 0.95']\n",
      "Processing Temp/images\\17bd5f92-drawing_106.png\n",
      "['LABEL_2 0.95', 'LABEL_0 0.88', 'LABEL_3 0.82', 'LABEL_2 0.94', 'LABEL_2 0.95', 'LABEL_1 0.82']\n",
      "Processing Temp/images\\2067578c-drawing_24.png\n",
      "['LABEL_2 0.89', 'LABEL_3 0.92', 'LABEL_0 0.87', 'LABEL_2 0.85']\n",
      "Processing Temp/images\\2466f98b-drawing_67.png\n",
      "['LABEL_0 0.83', 'LABEL_1 0.83', 'LABEL_2 0.91', 'LABEL_2 0.97', 'LABEL_0 0.87', 'LABEL_0 0.77', 'LABEL_0 0.71', 'LABEL_3 0.61', 'LABEL_2 0.94']\n",
      "Processing Temp/images\\3108be30-drawing_43.png\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[236], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMAGE_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[235], line 26\u001b[0m, in \u001b[0;36minference\u001b[1;34m(image_folder, CONFIDENCE_THRESHOLD, IOU_THRESHOLD)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# print(target)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 26\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Post-process\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     target_sizes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]])\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:1441\u001b[0m, in \u001b[0;36mDetrForObjectDetection.forward\u001b[1;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1438\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1440\u001b[0m \u001b[38;5;66;03m# First, sent images through DETR base model to obtain encoder + decoder outputs\u001b[39;00m\n\u001b[1;32m-> 1441\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1453\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;66;03m# class logits + predicted bounding boxes\u001b[39;00m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:1276\u001b[0m, in \u001b[0;36mDetrModel.forward\u001b[1;34m(self, pixel_values, pixel_mask, decoder_attention_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     pixel_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(((batch_size, height, width)), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;66;03m# First, sent pixel_values + pixel_mask through Backbone to obtain the features\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;66;03m# pixel_values should be of shape (batch_size, num_channels, height, width)\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;66;03m# pixel_mask should be of shape (batch_size, height, width)\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m features, object_queries_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;66;03m# get final feature map and downsampled mask\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m feature_map, mask \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:417\u001b[0m, in \u001b[0;36mDetrConvModel.forward\u001b[1;34m(self, pixel_values, pixel_mask)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, pixel_mask):\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# send pixel_values and pixel_mask through backbone to get list of (feature_map, pixel_mask) tuples\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m     pos \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_map, mask \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[0;32m    420\u001b[0m         \u001b[38;5;66;03m# position encoding\u001b[39;00m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\transformers\\models\\detr\\modeling_detr.py:395\u001b[0m, in \u001b[0;36mDetrConvEncoder.forward\u001b[1;34m(self, pixel_values, pixel_mask)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor, pixel_mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;66;03m# send pixel_values through the model to get list of feature maps\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_timm_backbone \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(pixel_values)\u001b[38;5;241m.\u001b[39mfeature_maps\n\u001b[0;32m    397\u001b[0m     out \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_map \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;66;03m# downsample pixel_mask to match shape of corresponding feature_map\u001b[39;00m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\timm\\models\\_features.py:345\u001b[0m, in \u001b[0;36mFeatureListNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m (List[torch\u001b[38;5;241m.\u001b[39mTensor]):\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\timm\\models\\_features.py:299\u001b[0m, in \u001b[0;36mFeatureDictNet._collect\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    297\u001b[0m     x \u001b[38;5;241m=\u001b[39m module(x) \u001b[38;5;28;01mif\u001b[39;00m first_or_last_module \u001b[38;5;28;01melse\u001b[39;00m checkpoint(module, x)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers:\n\u001b[0;32m    302\u001b[0m     out_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Object Detection\\object_detection\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1498\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = inference(IMAGE_FOLDER, 0.5, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images\\\\e4488dcd-outfile163.png': {'scores': tensor([0.9003, 0.9461, 0.9580, 0.5659, 0.7615, 0.6443, 0.8718, 0.9393, 0.9363,\n",
       "          0.9125]),\n",
       "  'labels': tensor([1, 4, 3, 3, 3, 3, 3, 3, 3, 2]),\n",
       "  'boxes': tensor([[ 51.0844, 519.2147, 173.7808, 555.1384],\n",
       "          [ 38.4558,  69.7419,  84.7549, 111.7220],\n",
       "          [414.7876, 229.3402, 480.2212, 259.4869],\n",
       "          [264.4022, 417.2076, 295.4213, 436.8293],\n",
       "          [265.4215, 396.1916, 303.8947, 418.6251],\n",
       "          [ 76.8188, 427.2198, 133.6106, 445.9435],\n",
       "          [ 73.5163, 421.4941, 139.9279, 448.7569],\n",
       "          [234.8455, 163.1137, 298.5825, 187.6942],\n",
       "          [616.2727, 289.4289, 670.7613, 310.4396],\n",
       "          [477.8539, 510.3906, 572.5653, 556.4916]])},\n",
       " 'images\\\\eb6f4ea9-output_folder87.png': {'scores': tensor([0.8908, 0.9492, 0.8376, 0.9139]),\n",
       "  'labels': tensor([1, 4, 2, 3]),\n",
       "  'boxes': tensor([[ 56.9786, 524.5278, 151.3273, 551.2535],\n",
       "          [ 35.5197, 109.3372,  74.6260, 146.9118],\n",
       "          [250.2004, 487.5692, 337.4134, 525.2523],\n",
       "          [584.9478, 319.1580, 670.9647, 348.2847]])},\n",
       " 'images\\\\ed041666-outfile137.png': {'scores': tensor([0.9081, 0.8862, 0.9353, 0.9389, 0.9462, 0.7751, 0.8566]),\n",
       "  'labels': tensor([1, 2, 4, 3, 3, 3, 3]),\n",
       "  'boxes': tensor([[ 73.9310, 531.0005, 197.5408, 563.6968],\n",
       "          [644.4281, 418.8531, 731.2484, 461.7573],\n",
       "          [ 32.1204, 195.7504,  82.1117, 238.5646],\n",
       "          [218.7840, 139.4366, 296.0943, 164.9680],\n",
       "          [636.7198, 178.2152, 706.0124, 206.7020],\n",
       "          [583.3372, 442.4115, 648.6372, 471.1025],\n",
       "          [381.6789, 472.4816, 444.9798, 505.2237]])},\n",
       " 'images\\\\ee2b485d-outfile161.png': {'scores': tensor([0.8985, 0.7689, 0.7855, 0.8705, 0.8926, 0.8947, 0.5081]),\n",
       "  'labels': tensor([3, 3, 3, 4, 3, 2, 2]),\n",
       "  'boxes': tensor([[342.8904, 420.8215, 400.2759, 448.0073],\n",
       "          [ 47.8971,  67.2621, 111.0880,  95.1105],\n",
       "          [659.2479, 395.4013, 721.5790, 419.5886],\n",
       "          [ 37.1694, 289.2999,  86.3410, 335.2663],\n",
       "          [515.5146,  51.3853, 572.0817,  76.4473],\n",
       "          [417.0482, 509.7649, 505.6602, 552.4483],\n",
       "          [579.8325, 440.9874, 697.7037, 477.8667]])},\n",
       " 'images\\\\f04ac094-outfile144.png': {'scores': tensor([0.9088, 0.8865, 0.9432, 0.9136, 0.9598, 0.8651, 0.6898]),\n",
       "  'labels': tensor([1, 2, 4, 3, 3, 3, 3]),\n",
       "  'boxes': tensor([[ 60.0501, 518.4028, 180.4144, 553.9489],\n",
       "          [632.3444, 384.0008, 724.3047, 426.9511],\n",
       "          [ 37.6417,  66.1609,  84.0884, 108.8869],\n",
       "          [627.6263, 306.9779, 723.5915, 338.2146],\n",
       "          [349.9829, 163.5302, 455.9060, 194.5580],\n",
       "          [233.5987, 505.5337, 310.4634, 536.5253],\n",
       "          [305.1074, 476.0417, 379.5953, 507.0357]])},\n",
       " 'images\\\\f0a95b57-output_folder62.png': {'scores': tensor([0.9066, 0.7937, 0.9357, 0.8481, 0.8856]),\n",
       "  'labels': tensor([1, 4, 3, 3, 2]),\n",
       "  'boxes': tensor([[ 42.5248, 530.7188, 122.4326, 556.9425],\n",
       "          [413.0531,  67.9089, 454.8773, 106.6465],\n",
       "          [220.4962, 141.0035, 292.5916, 164.4384],\n",
       "          [384.7541, 473.9571, 443.3564, 504.7876],\n",
       "          [611.9130, 368.2580, 703.0772, 409.9937]])},\n",
       " 'images\\\\f1196e7e-image_page11.png': {'scores': tensor([0.9089, 0.8883, 0.8391, 0.7752, 0.8947, 0.7939, 0.9015]),\n",
       "  'labels': tensor([1, 2, 3, 3, 4, 3, 3]),\n",
       "  'boxes': tensor([[ 40.8372, 535.6891, 139.9694, 557.4673],\n",
       "          [616.5388, 394.0002, 706.6873, 434.4753],\n",
       "          [ 43.6042,  60.6863, 145.2146,  88.9480],\n",
       "          [140.6544, 509.8496, 212.9323, 540.3613],\n",
       "          [265.8307, 104.6122, 307.9524, 145.3171],\n",
       "          [139.7344, 511.0689, 208.9672, 539.9797],\n",
       "          [591.1575,  90.1037, 681.5678, 127.7755]])},\n",
       " 'images\\\\f1b96071-outfile176.png': {'scores': tensor([0.9053, 0.9343, 0.8696, 0.9220, 0.9560, 0.9616, 0.9192, 0.9198]),\n",
       "  'labels': tensor([1, 4, 3, 3, 3, 3, 3, 2]),\n",
       "  'boxes': tensor([[ 34.9998, 524.1867, 157.6974, 561.0350],\n",
       "          [ 45.1519,  62.9200,  91.7335, 106.5739],\n",
       "          [307.0761, 462.2719, 363.7892, 496.9637],\n",
       "          [ 95.1774, 229.6073, 145.8644, 254.9889],\n",
       "          [323.5604, 236.7563, 378.8232, 262.9719],\n",
       "          [462.0378, 240.2397, 521.0528, 265.6879],\n",
       "          [663.4142, 179.4836, 719.7458, 204.4463],\n",
       "          [444.2990, 493.1049, 542.6378, 539.9702]])},\n",
       " 'images\\\\f2e01b1a-outfile105.png': {'scores': tensor([0.9294, 0.9407, 0.9562, 0.8858]),\n",
       "  'labels': tensor([1, 4, 3, 3]),\n",
       "  'boxes': tensor([[ 45.4949, 529.6472, 144.0697, 553.7787],\n",
       "          [ 29.4100,  60.1994,  66.5915,  95.7000],\n",
       "          [614.1095, 223.1243, 705.1943, 254.9758],\n",
       "          [308.9278, 467.9318, 384.0435, 503.4490]])},\n",
       " 'images\\\\f2f7bb30-outfile150.png': {'scores': tensor([0.9389, 0.9286, 0.7967, 0.9363, 0.8969]),\n",
       "  'labels': tensor([4, 3, 3, 3, 2]),\n",
       "  'boxes': tensor([[ 44.6283,  58.9577,  93.6601, 103.7888],\n",
       "          [622.4679, 261.3348, 721.6621, 294.1813],\n",
       "          [209.9437, 514.6258, 297.5894, 549.9875],\n",
       "          [382.2267, 350.3261, 494.0534, 390.2073],\n",
       "          [583.5318, 371.2478, 680.9118, 416.5320]])},\n",
       " 'images\\\\f77555f2-output_folder75.png': {'scores': tensor([0.9403, 0.7796, 0.8614]),\n",
       "  'labels': tensor([4, 3, 2]),\n",
       "  'boxes': tensor([[113.0443, 134.1753, 155.0501, 173.8632],\n",
       "          [214.1648, 519.3279, 289.0611, 549.1516],\n",
       "          [549.7668, 365.1794, 638.4542, 406.7962]])},\n",
       " 'images\\\\f94e31f0-output_folder64.png': {'scores': tensor([0.8839, 0.8687, 0.6436, 0.9069, 0.8718]),\n",
       "  'labels': tensor([1, 4, 3, 3, 2]),\n",
       "  'boxes': tensor([[ 56.2067, 532.7588, 135.8097, 556.6223],\n",
       "          [124.4366,  64.1976, 163.9384, 103.3993],\n",
       "          [ 53.6020,  96.8621, 138.9122, 126.2126],\n",
       "          [233.5426,  91.2975, 305.5396, 122.7006],\n",
       "          [604.9922, 364.5688, 694.3018, 405.3343]])},\n",
       " 'images\\\\f9725cc8-image_page16.png': {'scores': tensor([0.9287, 0.8730, 0.9478, 0.9396, 0.9512, 0.9186]),\n",
       "  'labels': tensor([1, 2, 4, 3, 3, 3]),\n",
       "  'boxes': tensor([[ 45.2381, 525.6306, 150.9867, 546.5524],\n",
       "          [602.6447, 399.3580, 696.9453, 440.7018],\n",
       "          [ 33.3464,  83.8061,  75.1385, 123.9468],\n",
       "          [ 76.7286, 257.7095, 183.8030, 285.4374],\n",
       "          [586.9788, 225.9118, 677.6624, 260.0479],\n",
       "          [427.0847, 270.3921, 535.5862, 305.5685]])},\n",
       " 'images\\\\fae5fc59-outfile115.png': {'scores': tensor([0.8951, 0.7607, 0.9152, 0.8048, 0.7778, 0.9056, 0.9577]),\n",
       "  'labels': tensor([1, 4, 4, 2, 3, 3, 3]),\n",
       "  'boxes': tensor([[ 32.0620, 523.4439, 145.0571, 556.9550],\n",
       "          [ 98.3349,  77.3552, 156.8094, 118.7556],\n",
       "          [131.3922,  73.4732, 170.6266, 115.8701],\n",
       "          [239.2645, 501.6951, 326.6798, 542.0921],\n",
       "          [ 50.9084,  95.3751, 134.0194, 125.5428],\n",
       "          [233.3411,  90.2810, 307.4093, 123.0473],\n",
       "          [587.5610, 213.2446, 671.0410, 244.3743]])},\n",
       " 'images\\\\fcd497aa-outfile140.png': {'scores': tensor([0.8600, 0.8349, 0.8100, 0.8443, 0.9118, 0.9617, 0.8713]),\n",
       "  'labels': tensor([1, 3, 3, 4, 3, 3, 2]),\n",
       "  'boxes': tensor([[ 57.7843, 538.7556, 170.4099, 565.9702],\n",
       "          [ 42.1408,  58.8603, 147.1018,  89.2953],\n",
       "          [136.3071, 505.7846, 218.1247, 540.3334],\n",
       "          [ 31.6656, 316.9379,  78.7829, 366.2816],\n",
       "          [589.1924,  88.1443, 682.5137, 127.0954],\n",
       "          [395.8922, 266.2735, 501.9026, 302.5718],\n",
       "          [590.8383, 367.5861, 688.9695, 411.0579]])},\n",
       " 'images\\\\fd666743-outfile162.png': {'scores': tensor([0.8131, 0.9360, 0.8402, 0.9409, 0.6421, 0.6025, 0.9466, 0.7760, 0.8730]),\n",
       "  'labels': tensor([1, 4, 3, 3, 1, 1, 3, 3, 2]),\n",
       "  'boxes': tensor([[ 26.1416, 524.8467, 139.1445, 559.9083],\n",
       "          [ 29.3182, 192.3600,  79.0027, 235.3826],\n",
       "          [381.8486, 470.9503, 444.8036, 504.2260],\n",
       "          [218.2522, 138.7642, 296.7559, 164.7786],\n",
       "          [ 31.6117, 498.1091, 145.5153, 525.1520],\n",
       "          [ 35.4551, 513.2133, 144.3681, 534.7554],\n",
       "          [636.4484, 177.6809, 706.3645, 206.6478],\n",
       "          [581.3161, 443.0131, 660.5067, 473.1896],\n",
       "          [399.7773, 520.2274, 496.1973, 563.9677]])},\n",
       " 'images\\\\fd9c4878-outfile112.png': {'scores': tensor([0.9269, 0.8964, 0.7522, 0.8907, 0.8667, 0.8928, 0.8868]),\n",
       "  'labels': tensor([1, 3, 3, 3, 4, 3, 2]),\n",
       "  'boxes': tensor([[ 34.2906, 542.9016, 128.4902, 566.2792],\n",
       "          [343.9662, 421.2585, 402.0236, 447.9575],\n",
       "          [ 48.6877,  67.4900, 109.7035,  94.6219],\n",
       "          [656.8728, 394.9902, 721.7823, 418.6792],\n",
       "          [ 60.2587, 295.4848, 107.0977, 342.5472],\n",
       "          [516.4218,  51.1948, 572.2767,  75.9665],\n",
       "          [358.5008, 509.2072, 445.7259, 550.8494]])},\n",
       " 'images\\\\fe1da71c-output_folder52.png': {'scores': tensor([0.8976, 0.9600, 0.9453, 0.9552, 0.9091]),\n",
       "  'labels': tensor([1, 3, 4, 3, 2]),\n",
       "  'boxes': tensor([[ 39.6200, 509.5973, 156.9387, 544.9400],\n",
       "          [366.9648, 179.1461, 473.5695, 212.6205],\n",
       "          [ 59.6536,  71.0701, 104.4164, 111.3780],\n",
       "          [610.6157, 221.4497, 710.1091, 256.5091],\n",
       "          [585.3461, 375.7903, 677.9926, 419.9234]])}}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Predictions DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 6)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame()\n",
    "for k in results:\n",
    "    df = pd.DataFrame(results[k]['boxes'].detach().to('cpu').numpy(), columns=['x1', 'y1', 'x2', 'y2'])\n",
    "    df['labels'] = results[k]['labels'].detach().to('cpu').numpy()\n",
    "    df['image'] = k.replace('images\\\\', '')\n",
    "    # print(df['image'])\n",
    "    predictions_df = pd.concat([predictions_df, df], ignore_index=True)\n",
    "predictions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "2    51\n",
       "3    21\n",
       "1    20\n",
       "0    18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['11913d58-drawing_88.png', '17bd5f92-drawing_106.png',\n",
       "       '2067578c-drawing_24.png', '2466f98b-drawing_67.png',\n",
       "       '3108be30-drawing_43.png', '449d473c-drawing_25.png',\n",
       "       '4fdca17b-drawing_7.png', '5acab2e6-drawing_64.png',\n",
       "       '8318502d-drawing_15.png', '92e4c80c-drawing_45.png',\n",
       "       'a01b4a37-drawing_168.png', 'a34b8e8d-drawing_44.png',\n",
       "       'a55f419c-drawing_48.png', 'af0c0b68-drawing_76.png',\n",
       "       'b6c7e8be-drawing_56.png', 'cc5b592c-drawing_113.png',\n",
       "       'e616d66b-drawing_123.png', 'eef413ab-drawing_22.png',\n",
       "       'f601361a-drawing_42.png'], dtype=object)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df['image'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ground Truths DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 6)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df = pd.DataFrame()\n",
    "for k in ground_truth:\n",
    "    df = pd.DataFrame(ground_truth[k]['boxes'], columns=['x1', 'y1', 'x2', 'y2'])\n",
    "    df['labels'] = ground_truth[k]['labels']\n",
    "    # print(k)\n",
    "    df['image'] = k.replace('images/', '')\n",
    "    # print(df['image'])\n",
    "    ground_truth_df = pd.concat([ground_truth_df, df], ignore_index=True)\n",
    "    \n",
    "ground_truth_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "2    50\n",
       "0    27\n",
       "3    22\n",
       "1    17\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = ground_truth_df.sort_values(by=['image', 'labels', 'y1', 'x1'])\n",
    "predictions_df = predictions_df.sort_values(by=['image', 'labels', 'y1', 'x1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df['row_count'] = ground_truth_df.groupby(['image', 'labels']).cumcount() + 1\n",
    "predictions_df['row_count'] = predictions_df.groupby(['image', 'labels']).cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>labels</th>\n",
       "      <th>image</th>\n",
       "      <th>row_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>278.494934</td>\n",
       "      <td>528.815002</td>\n",
       "      <td>360.598358</td>\n",
       "      <td>558.032288</td>\n",
       "      <td>1</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>623.942871</td>\n",
       "      <td>267.716156</td>\n",
       "      <td>710.059937</td>\n",
       "      <td>292.252167</td>\n",
       "      <td>2</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216.318863</td>\n",
       "      <td>518.310669</td>\n",
       "      <td>292.160645</td>\n",
       "      <td>546.712708</td>\n",
       "      <td>2</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>214.266266</td>\n",
       "      <td>519.274414</td>\n",
       "      <td>292.641144</td>\n",
       "      <td>547.464417</td>\n",
       "      <td>2</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.732620</td>\n",
       "      <td>70.202827</td>\n",
       "      <td>80.939987</td>\n",
       "      <td>104.400940</td>\n",
       "      <td>3</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39.997841</td>\n",
       "      <td>352.875702</td>\n",
       "      <td>75.729424</td>\n",
       "      <td>385.811737</td>\n",
       "      <td>3</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.061279</td>\n",
       "      <td>513.471252</td>\n",
       "      <td>136.894226</td>\n",
       "      <td>533.420105</td>\n",
       "      <td>0</td>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>616.694702</td>\n",
       "      <td>385.592316</td>\n",
       "      <td>695.228088</td>\n",
       "      <td>415.344025</td>\n",
       "      <td>1</td>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>425.991577</td>\n",
       "      <td>109.794601</td>\n",
       "      <td>497.029144</td>\n",
       "      <td>136.679779</td>\n",
       "      <td>2</td>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>78.777405</td>\n",
       "      <td>263.343567</td>\n",
       "      <td>174.254745</td>\n",
       "      <td>290.355896</td>\n",
       "      <td>2</td>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1          y1          x2          y2  labels  \\\n",
       "0   278.494934  528.815002  360.598358  558.032288       1   \n",
       "5   623.942871  267.716156  710.059937  292.252167       2   \n",
       "2   216.318863  518.310669  292.160645  546.712708       2   \n",
       "3   214.266266  519.274414  292.641144  547.464417       2   \n",
       "1    46.732620   70.202827   80.939987  104.400940       3   \n",
       "4    39.997841  352.875702   75.729424  385.811737       3   \n",
       "7    48.061279  513.471252  136.894226  533.420105       0   \n",
       "11  616.694702  385.592316  695.228088  415.344025       1   \n",
       "9   425.991577  109.794601  497.029144  136.679779       2   \n",
       "10   78.777405  263.343567  174.254745  290.355896       2   \n",
       "\n",
       "                       image  row_count  \n",
       "0    11913d58-drawing_88.png          1  \n",
       "5    11913d58-drawing_88.png          1  \n",
       "2    11913d58-drawing_88.png          2  \n",
       "3    11913d58-drawing_88.png          3  \n",
       "1    11913d58-drawing_88.png          1  \n",
       "4    11913d58-drawing_88.png          2  \n",
       "7   17bd5f92-drawing_106.png          1  \n",
       "11  17bd5f92-drawing_106.png          1  \n",
       "9   17bd5f92-drawing_106.png          1  \n",
       "10  17bd5f92-drawing_106.png          2  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground_truth_df.head(10)\n",
    "predictions_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge The DataFrames for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(ground_truth_df, predictions_df, on=['image', 'labels', 'row_count'], how='outer', suffixes=('_gt', '_pred'), validate='many_to_many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1_gt</th>\n",
       "      <th>y1_gt</th>\n",
       "      <th>x2_gt</th>\n",
       "      <th>y2_gt</th>\n",
       "      <th>labels</th>\n",
       "      <th>image</th>\n",
       "      <th>row_count</th>\n",
       "      <th>x1_pred</th>\n",
       "      <th>y1_pred</th>\n",
       "      <th>x2_pred</th>\n",
       "      <th>y2_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>626.232558</td>\n",
       "      <td>409.581395</td>\n",
       "      <td>732.186047</td>\n",
       "      <td>429.348837</td>\n",
       "      <td>0</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>279.116279</td>\n",
       "      <td>529.767442</td>\n",
       "      <td>363.720930</td>\n",
       "      <td>559.023256</td>\n",
       "      <td>1</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>1</td>\n",
       "      <td>278.494934</td>\n",
       "      <td>528.815002</td>\n",
       "      <td>360.598358</td>\n",
       "      <td>558.032288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>626.232558</td>\n",
       "      <td>266.465116</td>\n",
       "      <td>713.209302</td>\n",
       "      <td>290.186047</td>\n",
       "      <td>2</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>1</td>\n",
       "      <td>623.942871</td>\n",
       "      <td>267.716156</td>\n",
       "      <td>710.059937</td>\n",
       "      <td>292.252167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>217.441860</td>\n",
       "      <td>518.697674</td>\n",
       "      <td>278.325581</td>\n",
       "      <td>542.418605</td>\n",
       "      <td>2</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>2</td>\n",
       "      <td>216.318863</td>\n",
       "      <td>518.310669</td>\n",
       "      <td>292.160645</td>\n",
       "      <td>546.712708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>3</td>\n",
       "      <td>214.266266</td>\n",
       "      <td>519.274414</td>\n",
       "      <td>292.641144</td>\n",
       "      <td>547.464417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46.651163</td>\n",
       "      <td>66.418605</td>\n",
       "      <td>83.813953</td>\n",
       "      <td>105.162791</td>\n",
       "      <td>3</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>1</td>\n",
       "      <td>46.732620</td>\n",
       "      <td>70.202827</td>\n",
       "      <td>80.939987</td>\n",
       "      <td>104.400940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39.534884</td>\n",
       "      <td>351.069767</td>\n",
       "      <td>75.116279</td>\n",
       "      <td>389.813953</td>\n",
       "      <td>3</td>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>2</td>\n",
       "      <td>39.997841</td>\n",
       "      <td>352.875702</td>\n",
       "      <td>75.729424</td>\n",
       "      <td>385.811737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46.651163</td>\n",
       "      <td>515.534884</td>\n",
       "      <td>136.790698</td>\n",
       "      <td>535.302326</td>\n",
       "      <td>0</td>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>1</td>\n",
       "      <td>48.061279</td>\n",
       "      <td>513.471252</td>\n",
       "      <td>136.894226</td>\n",
       "      <td>533.420105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>617.534884</td>\n",
       "      <td>386.651163</td>\n",
       "      <td>695.813953</td>\n",
       "      <td>419.860465</td>\n",
       "      <td>1</td>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>1</td>\n",
       "      <td>616.694702</td>\n",
       "      <td>385.592316</td>\n",
       "      <td>695.228088</td>\n",
       "      <td>415.344025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>426.976744</td>\n",
       "      <td>108.325581</td>\n",
       "      <td>498.139535</td>\n",
       "      <td>136.790698</td>\n",
       "      <td>2</td>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>1</td>\n",
       "      <td>425.991577</td>\n",
       "      <td>109.794601</td>\n",
       "      <td>497.029144</td>\n",
       "      <td>136.679779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>76.697674</td>\n",
       "      <td>260.930233</td>\n",
       "      <td>173.953488</td>\n",
       "      <td>285.441860</td>\n",
       "      <td>2</td>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>2</td>\n",
       "      <td>78.777405</td>\n",
       "      <td>263.343567</td>\n",
       "      <td>174.254745</td>\n",
       "      <td>290.355896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>430.139535</td>\n",
       "      <td>279.906977</td>\n",
       "      <td>529.767442</td>\n",
       "      <td>305.209302</td>\n",
       "      <td>2</td>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>3</td>\n",
       "      <td>428.577881</td>\n",
       "      <td>281.212646</td>\n",
       "      <td>526.546204</td>\n",
       "      <td>308.868805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1_gt       y1_gt       x2_gt       y2_gt  labels  \\\n",
       "0   626.232558  409.581395  732.186047  429.348837       0   \n",
       "1   279.116279  529.767442  363.720930  559.023256       1   \n",
       "2   626.232558  266.465116  713.209302  290.186047       2   \n",
       "3   217.441860  518.697674  278.325581  542.418605       2   \n",
       "4          NaN         NaN         NaN         NaN       2   \n",
       "5    46.651163   66.418605   83.813953  105.162791       3   \n",
       "6    39.534884  351.069767   75.116279  389.813953       3   \n",
       "7    46.651163  515.534884  136.790698  535.302326       0   \n",
       "8   617.534884  386.651163  695.813953  419.860465       1   \n",
       "9   426.976744  108.325581  498.139535  136.790698       2   \n",
       "10   76.697674  260.930233  173.953488  285.441860       2   \n",
       "11  430.139535  279.906977  529.767442  305.209302       2   \n",
       "\n",
       "                       image  row_count     x1_pred     y1_pred     x2_pred  \\\n",
       "0    11913d58-drawing_88.png          1         NaN         NaN         NaN   \n",
       "1    11913d58-drawing_88.png          1  278.494934  528.815002  360.598358   \n",
       "2    11913d58-drawing_88.png          1  623.942871  267.716156  710.059937   \n",
       "3    11913d58-drawing_88.png          2  216.318863  518.310669  292.160645   \n",
       "4    11913d58-drawing_88.png          3  214.266266  519.274414  292.641144   \n",
       "5    11913d58-drawing_88.png          1   46.732620   70.202827   80.939987   \n",
       "6    11913d58-drawing_88.png          2   39.997841  352.875702   75.729424   \n",
       "7   17bd5f92-drawing_106.png          1   48.061279  513.471252  136.894226   \n",
       "8   17bd5f92-drawing_106.png          1  616.694702  385.592316  695.228088   \n",
       "9   17bd5f92-drawing_106.png          1  425.991577  109.794601  497.029144   \n",
       "10  17bd5f92-drawing_106.png          2   78.777405  263.343567  174.254745   \n",
       "11  17bd5f92-drawing_106.png          3  428.577881  281.212646  526.546204   \n",
       "\n",
       "       y2_pred  \n",
       "0          NaN  \n",
       "1   558.032288  \n",
       "2   292.252167  \n",
       "3   546.712708  \n",
       "4   547.464417  \n",
       "5   104.400940  \n",
       "6   385.811737  \n",
       "7   533.420105  \n",
       "8   415.344025  \n",
       "9   136.679779  \n",
       "10  290.355896  \n",
       "11  308.868805  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
    "    \n",
    "    Parameters:\n",
    "    - box1: (x1, y1, x2, y2) coordinates of the first bounding box\n",
    "    - box2: (x1, y1, x2, y2) coordinates of the second bounding box\n",
    "    \n",
    "    Returns:\n",
    "    - iou: Intersection over Union (IoU) value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the coordinates of the two boxes\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2\n",
    "    \n",
    "    # Calculate the (x, y) coordinates of the intersection rectangle\n",
    "    xi1 = max(x1_1, x1_2)\n",
    "    yi1 = max(y1_1, y1_2)\n",
    "    xi2 = min(x2_1, x2_2)\n",
    "    yi2 = min(y2_1, y2_2)\n",
    "    \n",
    "    # Calculate the area of the intersection rectangle\n",
    "    inter_width = max(0, xi2 - xi1)\n",
    "    inter_height = max(0, yi2 - yi1)\n",
    "    inter_area = inter_width * inter_height\n",
    "    \n",
    "    # Calculate the area of both bounding boxes\n",
    "    box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    \n",
    "    # Calculate the union area\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    # Calculate the IoU\n",
    "    iou = inter_area / union_area if union_area != 0 else 0\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['iou'] = merged_df.apply(lambda x: calculate_iou((x['x1_gt'], x['y1_gt'], x['x2_gt'], x['y2_gt']), (x['x1_pred'], x['y1_pred'], x['x2_pred'], x['y2_pred'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['11913d58-drawing_88.png', '17bd5f92-drawing_106.png',\n",
       "       '2067578c-drawing_24.png', '2466f98b-drawing_67.png',\n",
       "       '3108be30-drawing_43.png', '449d473c-drawing_25.png',\n",
       "       '4fdca17b-drawing_7.png', '5acab2e6-drawing_64.png',\n",
       "       '8318502d-drawing_15.png', '92e4c80c-drawing_45.png',\n",
       "       'a01b4a37-drawing_168.png', 'a34b8e8d-drawing_44.png',\n",
       "       'a55f419c-drawing_48.png', 'af0c0b68-drawing_76.png',\n",
       "       'b6c7e8be-drawing_56.png', 'cc5b592c-drawing_113.png',\n",
       "       'e616d66b-drawing_123.png', 'eef413ab-drawing_22.png',\n",
       "       'f601361a-drawing_42.png'], dtype=object)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merged_df.head(4)\n",
    "# merged_df.loc[merged_df['image'] == 'e616d66b-drawing_123.png']\n",
    "merged_df['image'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Negative\n",
      "True Positive\n",
      "IoU 1: 0.8958\n",
      "True Positive\n",
      "IoU 2: 0.8216\n",
      "True Positive\n",
      "IoU 3: 0.6705\n",
      "False Positive\n",
      "True Positive\n",
      "IoU 5: 0.8125\n",
      "True Positive\n",
      "IoU 6: 0.8269\n",
      "True Positive\n",
      "IoU 7: 0.8068\n",
      "True Positive\n",
      "IoU 8: 0.8236\n",
      "True Positive\n",
      "IoU 9: 0.9178\n",
      "True Positive\n",
      "IoU 10: 0.7355\n",
      "True Positive\n",
      "IoU 11: 0.7931\n",
      "True Positive\n",
      "IoU 12: 0.8350\n",
      "False Positive\n",
      "IoU 13: 0.0000\n",
      "False Negative\n",
      "True Positive\n",
      "IoU 15: 0.8376\n",
      "True Positive\n",
      "IoU 16: 0.8870\n",
      "True Positive\n",
      "IoU 17: 0.7583\n",
      "False Positive\n",
      "IoU 18: 0.0000\n",
      "False Positive\n",
      "IoU 19: 0.0000\n",
      "False Positive\n",
      "IoU 20: 0.0000\n",
      "False Positive\n",
      "IoU 21: 0.0000\n",
      "True Positive\n",
      "IoU 22: 0.6801\n",
      "True Positive\n",
      "IoU 23: 0.9080\n",
      "True Positive\n",
      "IoU 24: 0.6815\n",
      "True Positive\n",
      "IoU 25: 0.8480\n",
      "True Positive\n",
      "IoU 26: 0.7552\n",
      "True Positive\n",
      "IoU 27: 0.7710\n",
      "True Positive\n",
      "IoU 28: 0.8548\n",
      "True Positive\n",
      "IoU 29: 0.8003\n",
      "True Positive\n",
      "IoU 30: 0.7402\n",
      "True Positive\n",
      "IoU 31: 0.8353\n",
      "True Positive\n",
      "IoU 32: 0.9389\n",
      "True Positive\n",
      "IoU 33: 0.8790\n",
      "False Positive\n",
      "IoU 34: 0.5982\n",
      "True Positive\n",
      "IoU 35: 0.6150\n",
      "True Positive\n",
      "IoU 36: 0.6868\n",
      "True Positive\n",
      "IoU 37: 0.8391\n",
      "True Positive\n",
      "IoU 38: 0.8606\n",
      "True Positive\n",
      "IoU 39: 0.7737\n",
      "False Negative\n",
      "True Positive\n",
      "IoU 41: 0.6664\n",
      "False Positive\n",
      "True Positive\n",
      "IoU 43: 0.7966\n",
      "True Positive\n",
      "IoU 44: 0.7838\n",
      "True Positive\n",
      "IoU 45: 0.7529\n",
      "True Positive\n",
      "IoU 46: 0.7443\n",
      "True Positive\n",
      "IoU 47: 0.8310\n",
      "True Positive\n",
      "IoU 48: 0.8922\n",
      "True Positive\n",
      "IoU 49: 0.7039\n",
      "True Positive\n",
      "IoU 50: 0.8408\n",
      "True Positive\n",
      "IoU 51: 0.7800\n",
      "True Positive\n",
      "IoU 52: 0.7798\n",
      "True Positive\n",
      "IoU 53: 0.7263\n",
      "True Positive\n",
      "IoU 54: 0.8052\n",
      "True Positive\n",
      "IoU 55: 0.8778\n",
      "True Positive\n",
      "IoU 56: 0.7682\n",
      "True Positive\n",
      "IoU 57: 0.7323\n",
      "True Positive\n",
      "IoU 58: 0.7346\n",
      "True Positive\n",
      "IoU 59: 0.7376\n",
      "True Positive\n",
      "IoU 60: 0.7173\n",
      "True Positive\n",
      "IoU 61: 0.8023\n",
      "False Negative\n",
      "False Negative\n",
      "True Positive\n",
      "IoU 64: 0.8337\n",
      "True Positive\n",
      "IoU 65: 0.7371\n",
      "False Positive\n",
      "True Positive\n",
      "IoU 67: 0.7362\n",
      "False Negative\n",
      "False Positive\n",
      "IoU 69: 0.1204\n",
      "False Negative\n",
      "True Positive\n",
      "IoU 71: 0.7905\n",
      "True Positive\n",
      "IoU 72: 0.7884\n",
      "True Positive\n",
      "IoU 73: 0.7962\n",
      "True Positive\n",
      "IoU 74: 0.7712\n",
      "True Positive\n",
      "IoU 75: 0.8631\n",
      "True Positive\n",
      "IoU 76: 0.6997\n",
      "True Positive\n",
      "IoU 77: 0.7640\n",
      "True Positive\n",
      "IoU 78: 0.7675\n",
      "True Positive\n",
      "IoU 79: 0.9329\n",
      "True Positive\n",
      "IoU 80: 0.8810\n",
      "True Positive\n",
      "IoU 81: 0.7793\n",
      "True Positive\n",
      "IoU 82: 0.7843\n",
      "True Positive\n",
      "IoU 83: 0.7807\n",
      "True Positive\n",
      "IoU 84: 0.8910\n",
      "True Positive\n",
      "IoU 85: 0.8231\n",
      "True Positive\n",
      "IoU 86: 0.7805\n",
      "True Positive\n",
      "IoU 87: 0.7975\n",
      "True Positive\n",
      "IoU 88: 0.8675\n",
      "True Positive\n",
      "IoU 89: 0.8174\n",
      "True Positive\n",
      "IoU 90: 0.8523\n",
      "True Positive\n",
      "IoU 91: 0.6120\n",
      "True Positive\n",
      "IoU 92: 0.6828\n",
      "True Positive\n",
      "IoU 93: 0.8127\n",
      "True Positive\n",
      "IoU 94: 0.8722\n",
      "True Positive\n",
      "IoU 95: 0.8209\n",
      "True Positive\n",
      "IoU 96: 0.8388\n",
      "True Positive\n",
      "IoU 97: 0.7920\n",
      "True Positive\n",
      "IoU 98: 0.8719\n",
      "False Negative\n",
      "False Negative\n",
      "False Positive\n",
      "IoU 101: 0.0000\n",
      "False Positive\n",
      "True Positive\n",
      "IoU 103: 0.7955\n",
      "False Negative\n",
      "True Positive\n",
      "IoU 105: 0.7882\n",
      "False Negative\n",
      "True Positive\n",
      "IoU 107: 0.7915\n",
      "False Positive\n",
      "True Positive\n",
      "IoU 109: 0.8819\n",
      "True Positive\n",
      "IoU 110: 0.8767\n",
      "True Positive\n",
      "IoU 111: 0.8258\n",
      "True Positive\n",
      "IoU 112: 0.6309\n",
      "True Positive\n",
      "IoU 113: 0.9129\n",
      "True Positive\n",
      "IoU 114: 0.7417\n",
      "True Positive\n",
      "IoU 115: 0.8538\n",
      "True Positive\n",
      "IoU 116: 0.6771\n",
      "True Positive\n",
      "IoU 117: 0.6583\n",
      "True Positive\n",
      "IoU 118: 0.7945\n",
      "True Positive\n",
      "IoU 119: 0.8862\n",
      "True Positive\n",
      "IoU 120: 0.7814\n"
     ]
    }
   ],
   "source": [
    "TP, FP, FN = 0, 0, 0\n",
    "merged_df['result'] = ''\n",
    "for i in range(merged_df.shape[0]):\n",
    "    # if merged_df['image'][i] == '5acab2e6-drawing_64.png':\n",
    "    if merged_df['iou'][i] > 0.6:\n",
    "        print('True Positive')\n",
    "        print(f\"IoU {i}: {merged_df['iou'][i]:.4f}\")\n",
    "        TP += 1\n",
    "        # df.loc[row_indexer, \"col\"] = values\n",
    "        merged_df.loc[i, 'result'] = 'TP'\n",
    "        # merged_df['result'][i] = 'TP'\n",
    "    elif merged_df['iou'][i] == 0:\n",
    "        print('False Positive')\n",
    "        print(f\"IoU {i}: {merged_df['iou'][i]:.4f}\")\n",
    "        FP += 1\n",
    "        merged_df.loc[i, 'result'] = 'FP'\n",
    "    elif pd.isna(merged_df['iou'][i]):\n",
    "        if pd.isna(merged_df['x1_pred'][i]):\n",
    "            print('False Negative')\n",
    "            FN += 1\n",
    "            merged_df.loc[i, 'result'] = 'FN'\n",
    "        if pd.isna(merged_df['x1_gt'][i]):\n",
    "            print('False Positive')\n",
    "            FP += 1\n",
    "            merged_df.loc[i, 'result'] = 'FP'\n",
    "    else: \n",
    "        print('False Positive')\n",
    "        print(f\"IoU {i}: {merged_df['iou'][i]:.4f}\")\n",
    "        FP += 1\n",
    "        merged_df.loc[i, 'result'] = 'FP'\n",
    "    # merged_df['result'][i] = 'FP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1_gt</th>\n",
       "      <th>y1_gt</th>\n",
       "      <th>x2_gt</th>\n",
       "      <th>y2_gt</th>\n",
       "      <th>labels</th>\n",
       "      <th>image</th>\n",
       "      <th>row_count</th>\n",
       "      <th>x1_pred</th>\n",
       "      <th>y1_pred</th>\n",
       "      <th>x2_pred</th>\n",
       "      <th>y2_pred</th>\n",
       "      <th>iou</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [x1_gt, y1_gt, x2_gt, y2_gt, labels, image, row_count, x1_pred, y1_pred, x2_pred, y2_pred, iou, result]\n",
       "Index: []"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df['image'] == 'fae5fc59-outfile115.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive -  97\n",
      "False Positive -  13\n",
      "False Negative -  11\n"
     ]
    }
   ],
   "source": [
    "print(\"True Positive - \", TP)\n",
    "print(\"False Positive - \", FP)\n",
    "print(\"False Negative - \", FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy -  0.8016528925619835\n",
      "Precision -  0.8818181818181818\n",
      "Recall -  0.8981481481481481\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy - \", TP/(TP+FP+FN))\n",
    "print(\"Precision - \", TP/(TP+FP))\n",
    "print(\"Recall - \", TP/(TP+FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1_gt</th>\n",
       "      <th>y1_gt</th>\n",
       "      <th>x2_gt</th>\n",
       "      <th>y2_gt</th>\n",
       "      <th>labels</th>\n",
       "      <th>image</th>\n",
       "      <th>row_count</th>\n",
       "      <th>x1_pred</th>\n",
       "      <th>y1_pred</th>\n",
       "      <th>x2_pred</th>\n",
       "      <th>y2_pred</th>\n",
       "      <th>iou</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [x1_gt, y1_gt, x2_gt, y2_gt, labels, image, row_count, x1_pred, y1_pred, x2_pred, y2_pred, iou, result]\n",
       "Index: []"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.loc[merged_df['image'] == \"ee2b485d-outfile161.png\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Wise Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Scale (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'bar-scale', 1: 'color-stamp', 2: 'detail-labels', 3: 'north-sign'}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FN, FP = merged_df.loc[merged_df['labels']==0]['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.4444\n",
      "Precision = 0.6667\n",
      "Recall = 0.5714\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {TP/(TP+FP+FN):.4f}\")\n",
    "print(f\"Precision = {TP/(TP+FP):.4f}\")\n",
    "print(f\"Recall = {TP/(TP+FN):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Stamp (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP = merged_df.loc[merged_df['labels']==1]['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "FN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8000\n",
      "Precision = 0.8000\n",
      "Recall = 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {TP/(TP+FP+FN):.4f}\")\n",
    "print(f\"Precision = {TP/(TP+FP):.4f}\")\n",
    "print(f\"Recall = {TP/(TP+FN):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detail Labels (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FP, FN = merged_df.loc[merged_df['labels']==2]['result'].value_counts()\n",
    "# FN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9231\n",
      "Precision = 0.9412\n",
      "Recall = 0.9796\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {TP/(TP+FP+FN):.4f}\")\n",
    "print(f\"Precision = {TP/(TP+FP):.4f}\")\n",
    "print(f\"Recall = {TP/(TP+FN):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### North Sign (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP, FN = merged_df.loc[merged_df['labels']==3]['result'].value_counts()\n",
    "FP = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9545\n",
      "Precision = 1.0000\n",
      "Recall = 0.9545\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {TP/(TP+FP+FN):.4f}\")\n",
    "print(f\"Precision = {TP/(TP+FP):.4f}\")\n",
    "print(f\"Recall = {TP/(TP+FN):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Labels in the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2067578c-drawing_24.png</td>\n",
       "      <td>{0, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2466f98b-drawing_67.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3108be30-drawing_43.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>449d473c-drawing_25.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4fdca17b-drawing_7.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5acab2e6-drawing_64.png</td>\n",
       "      <td>{0, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8318502d-drawing_15.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>92e4c80c-drawing_45.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a01b4a37-drawing_168.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a34b8e8d-drawing_44.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>a55f419c-drawing_48.png</td>\n",
       "      <td>{0, 1, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>af0c0b68-drawing_76.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b6c7e8be-drawing_56.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cc5b592c-drawing_113.png</td>\n",
       "      <td>{0, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e616d66b-drawing_123.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>eef413ab-drawing_22.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>f601361a-drawing_42.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image        labels\n",
       "0    11913d58-drawing_88.png     {1, 2, 3}\n",
       "1   17bd5f92-drawing_106.png  {0, 1, 2, 3}\n",
       "2    2067578c-drawing_24.png     {0, 2, 3}\n",
       "3    2466f98b-drawing_67.png  {0, 1, 2, 3}\n",
       "4    3108be30-drawing_43.png  {0, 1, 2, 3}\n",
       "5    449d473c-drawing_25.png  {0, 1, 2, 3}\n",
       "6     4fdca17b-drawing_7.png     {1, 2, 3}\n",
       "7    5acab2e6-drawing_64.png     {0, 2, 3}\n",
       "8    8318502d-drawing_15.png  {0, 1, 2, 3}\n",
       "9    92e4c80c-drawing_45.png  {0, 1, 2, 3}\n",
       "10  a01b4a37-drawing_168.png     {1, 2, 3}\n",
       "11   a34b8e8d-drawing_44.png  {0, 1, 2, 3}\n",
       "12   a55f419c-drawing_48.png     {0, 1, 3}\n",
       "13   af0c0b68-drawing_76.png  {0, 1, 2, 3}\n",
       "14   b6c7e8be-drawing_56.png  {0, 1, 2, 3}\n",
       "15  cc5b592c-drawing_113.png     {0, 2, 3}\n",
       "16  e616d66b-drawing_123.png     {1, 2, 3}\n",
       "17   eef413ab-drawing_22.png     {1, 2, 3}\n",
       "18   f601361a-drawing_42.png  {0, 1, 2, 3}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions_df.groupby('image')['labels'].unique()\n",
    "labels_df = predictions_df.groupby('image')['labels'].apply(lambda x: set(x)).reset_index()\n",
    "labels_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = {0, 1, 2, 3}\n",
    "\n",
    "labels_df['missing_labels'] = labels_df['labels'].apply(lambda x: all_labels - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>labels</th>\n",
       "      <th>missing_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11913d58-drawing_88.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17bd5f92-drawing_106.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2067578c-drawing_24.png</td>\n",
       "      <td>{0, 2, 3}</td>\n",
       "      <td>{1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2466f98b-drawing_67.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3108be30-drawing_43.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>449d473c-drawing_25.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4fdca17b-drawing_7.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5acab2e6-drawing_64.png</td>\n",
       "      <td>{0, 2, 3}</td>\n",
       "      <td>{1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8318502d-drawing_15.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>92e4c80c-drawing_45.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a01b4a37-drawing_168.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a34b8e8d-drawing_44.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>a55f419c-drawing_48.png</td>\n",
       "      <td>{0, 1, 3}</td>\n",
       "      <td>{2}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>af0c0b68-drawing_76.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b6c7e8be-drawing_56.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cc5b592c-drawing_113.png</td>\n",
       "      <td>{0, 2, 3}</td>\n",
       "      <td>{1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e616d66b-drawing_123.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>eef413ab-drawing_22.png</td>\n",
       "      <td>{1, 2, 3}</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>f601361a-drawing_42.png</td>\n",
       "      <td>{0, 1, 2, 3}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image        labels missing_labels\n",
       "0    11913d58-drawing_88.png     {1, 2, 3}            {0}\n",
       "1   17bd5f92-drawing_106.png  {0, 1, 2, 3}             {}\n",
       "2    2067578c-drawing_24.png     {0, 2, 3}            {1}\n",
       "3    2466f98b-drawing_67.png  {0, 1, 2, 3}             {}\n",
       "4    3108be30-drawing_43.png  {0, 1, 2, 3}             {}\n",
       "5    449d473c-drawing_25.png  {0, 1, 2, 3}             {}\n",
       "6     4fdca17b-drawing_7.png     {1, 2, 3}            {0}\n",
       "7    5acab2e6-drawing_64.png     {0, 2, 3}            {1}\n",
       "8    8318502d-drawing_15.png  {0, 1, 2, 3}             {}\n",
       "9    92e4c80c-drawing_45.png  {0, 1, 2, 3}             {}\n",
       "10  a01b4a37-drawing_168.png     {1, 2, 3}            {0}\n",
       "11   a34b8e8d-drawing_44.png  {0, 1, 2, 3}             {}\n",
       "12   a55f419c-drawing_48.png     {0, 1, 3}            {2}\n",
       "13   af0c0b68-drawing_76.png  {0, 1, 2, 3}             {}\n",
       "14   b6c7e8be-drawing_56.png  {0, 1, 2, 3}             {}\n",
       "15  cc5b592c-drawing_113.png     {0, 2, 3}            {1}\n",
       "16  e616d66b-drawing_123.png     {1, 2, 3}            {0}\n",
       "17   eef413ab-drawing_22.png     {1, 2, 3}            {0}\n",
       "18   f601361a-drawing_42.png  {0, 1, 2, 3}             {}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Labels in Training Data - 116\n",
      "Bar Scale Count - 27\n",
      "Color Stamp Count - 17\n",
      "Detail Label Count - 50\n",
      "North Sign Count - 22\n"
     ]
    }
   ],
   "source": [
    "# Ground Truth\n",
    "print(f'Number of Labels in Training Data - {ground_truth_df.shape[0]}')\n",
    "print(f'Bar Scale Count - {ground_truth_df.loc[ground_truth_df[\"labels\"]==0].shape[0]}')\n",
    "print(f'Color Stamp Count - {ground_truth_df.loc[ground_truth_df[\"labels\"]==1].shape[0]}')\n",
    "print(f'Detail Label Count - {ground_truth_df.loc[ground_truth_df[\"labels\"]==2].shape[0]}')\n",
    "print(f'North Sign Count - {ground_truth_df.loc[ground_truth_df[\"labels\"]==3].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Labels in Predictions - 110\n",
      "Bar Scale Count - 18\n",
      "Color Stamp Count - 20\n",
      "Detail Label Count - 51\n",
      "North Sign Count - 21\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "print(f'Number of Labels in Predictions - {predictions_df.shape[0]}')\n",
    "print(f'Bar Scale Count - {predictions_df.loc[predictions_df[\"labels\"]==0].shape[0]}')\n",
    "print(f'Color Stamp Count - {predictions_df.loc[predictions_df[\"labels\"]==1].shape[0]}')\n",
    "print(f'Detail Label Count - {predictions_df.loc[predictions_df[\"labels\"]==2].shape[0]}')\n",
    "print(f'North Sign Count - {predictions_df.loc[predictions_df[\"labels\"]==3].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from seaborn) (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\object detection\\object_detection\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # DETR authors employ various image sizes during training, making it not possible\n",
    "    # to directly batch together images. Hence they pad the images to the biggest\n",
    "    # resolution in a given batch, and create a corresponding binary pixel_mask\n",
    "    # which indicates which pixels are real/which are padding\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {\n",
    "        'pixel_values': encoding['pixel_values'],\n",
    "        'pixel_mask': encoding['pixel_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
    "VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=4)\n",
    "TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  20%|        | 1/5 [00:14<00:57, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch true labels: [array([3, 1, 0, 2], dtype=int64), array([3, 1, 0, 2, 2], dtype=int64), array([3, 1, 0, 2, 2, 2], dtype=int64), array([3, 0, 0, 2, 2], dtype=int64)]\n",
      "Batch predicted labels: [1 3 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  40%|      | 2/5 [00:36<00:56, 18.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch true labels: [array([3, 1, 0, 2, 2, 2, 2, 2], dtype=int64), array([3, 1, 1, 0, 2, 2, 2, 2], dtype=int64), array([3, 1, 0, 2, 2], dtype=int64), array([3, 1, 0, 0, 2], dtype=int64)]\n",
      "Batch predicted labels: [0 2 3 2 2 2 1 2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  60%|    | 3/5 [00:49<00:32, 16.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch true labels: [array([3, 1, 0, 2, 2, 2, 2], dtype=int64), array([3, 1, 0], dtype=int64), array([3, 1, 0, 0, 2, 2], dtype=int64), array([3, 0, 2, 2, 2], dtype=int64)]\n",
      "Batch predicted labels: [2 3 1 0 2 2 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:  80%|  | 4/5 [01:03<00:15, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch true labels: [array([3, 1, 0, 0, 0, 0, 2, 2, 2], dtype=int64), array([3, 3, 1, 0, 2, 2, 2, 2, 2, 2, 2], dtype=int64), array([3, 3, 1, 0, 2, 2], dtype=int64), array([3, 1, 0, 2, 2, 2], dtype=int64)]\n",
      "Batch predicted labels: [0 1 2 2 0 0 0 3 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 5/5 [01:19<00:00, 15.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch true labels: [array([3, 0, 2, 2, 2], dtype=int64), array([3, 1, 0, 0, 2, 2], dtype=int64), array([3, 3, 1, 0, 0, 2], dtype=int64)]\n",
      "Batch predicted labels: [0 3 2 2 2]\n",
      "Time required for testing: 79.03 seconds\n",
      "True labels: [3 1 0 2 3 1 0 2 2 3 1 0 2 2 2 3 0 0 2 2 3 1 0 2 2 2 2 2 3 1 1 0 2 2]\n",
      "Predicted labels: [1 3 1 2 0 2 3 2 2 2 1 2 2 2 3 1 0 2 2 2 0 1 2 2 0 0 0 3 2 0 3 2 2 2]\n",
      "Accuracy: 0.3824\n",
      "Precision: 0.2828\n",
      "Recall: 0.2857\n",
      "F1 Score: 0.2829\n",
      "Error Rate: 0.6176\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    bar-scale       0.14      0.14      0.14         7\n",
      "  color-stamp       0.40      0.33      0.36         6\n",
      "detail-labels       0.59      0.67      0.62        15\n",
      "   north-sign       0.00      0.00      0.00         6\n",
      "\n",
      "     accuracy                           0.38        34\n",
      "    macro avg       0.28      0.29      0.28        34\n",
      " weighted avg       0.36      0.38      0.37        34\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def convert_to_detections(outputs, threshold=0.4):\n",
    "    probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "    keep = probas.max(-1).values > threshold\n",
    "    bboxes_scaled = outputs.pred_boxes[0, keep].detach().cpu().numpy()\n",
    "    probas = probas[keep].detach().cpu().numpy()\n",
    "    return bboxes_scaled, probas\n",
    " \n",
    "true_labels = []\n",
    "pred_labels = []\n",
    " \n",
    "test_time_start = time.time()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(TEST_DATALOADER, desc=\"Testing\"):\n",
    "        pixel_values = batch['pixel_values']\n",
    "        pixel_mask = batch.get('pixel_mask', None)\n",
    " \n",
    "        labels = batch['labels']\n",
    "\n",
    "        \n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "        bboxes, probas = convert_to_detections(outputs)\n",
    " \n",
    "        # Get predicted labels\n",
    "        batch_pred_labels = np.argmax(probas, axis=1)\n",
    "        pred_labels.extend(batch_pred_labels)\n",
    " \n",
    "        # Get true labels\n",
    "        for ann in labels:\n",
    "            if 'class_labels' in ann:\n",
    "                true_labels.extend(ann['class_labels'].cpu().numpy())\n",
    "            elif 'labels' in ann:\n",
    "                true_labels.extend(ann['labels'].cpu().numpy())\n",
    "            else:\n",
    "                print(\"Warning: No label information found in annotation.\")\n",
    "                print(\"Annotation keys:\", ann.keys())\n",
    " \n",
    "        # Debugging: Print the current batch's true and predicted labels\n",
    "        print(f\"Batch true labels: {[ann['class_labels'].cpu().numpy() if 'class_labels' in ann else ann['labels'].cpu().numpy() for ann in labels]}\")\n",
    "        print(f\"Batch predicted labels: {batch_pred_labels}\")\n",
    " \n",
    "test_time_end = time.time()\n",
    "total_time = test_time_end - test_time_start\n",
    "print(f\"Time required for testing: {total_time:.2f} seconds\")\n",
    " \n",
    "true_labels = np.array(true_labels)\n",
    "pred_labels = np.array(pred_labels)\n",
    " \n",
    "# Ensure true_labels and pred_labels have the same length\n",
    "min_len = min(len(true_labels), len(pred_labels))\n",
    "true_labels = true_labels[:min_len]\n",
    "pred_labels = pred_labels[:min_len]\n",
    " \n",
    "print(\"True labels:\", true_labels)\n",
    "print(\"Predicted labels:\", pred_labels)\n",
    " \n",
    "# Define id2label mapping\n",
    "id2label = {\n",
    "    0: \"bar-scale\",\n",
    "    1: \"color-stamp\",\n",
    "    2: \"detail-labels\",\n",
    "    3: \"north-sign\"\n",
    "}\n",
    " \n",
    "# Calculate and print classification metrics\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')\n",
    "recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "error_rate = 1 - accuracy\n",
    " \n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Error Rate: {error_rate:.4f}\")\n",
    "print(\"\\n\")\n",
    " \n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, pred_labels, target_names=list(id2label.values())))\n",
    " \n",
    "# Create and plot confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels, labels=list(id2label.keys()))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=list(id2label.values()), yticklabels=list(id2label.values()))\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
